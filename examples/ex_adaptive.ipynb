{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of fixed grid Fourier integration\n",
    "\n",
    "This notebook shows how `neffint.fourier_integral_fixed_sampling` can be used to calculate fourier integrals. This function is well suited for computing the fourier integrals of functions on a already defined frequency grid which captures all important features of the function to be transformed. More precisely, interpolating the function over the the given frequencies should give an interpolating polynomial which closely approximates the function itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "import neffint as nft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some functions to measure error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_diff(x: float, y: float) -> float:\n",
    "    \"\"\"Relative difference\"\"\"\n",
    "    if max(abs(x), abs(y)) == 0:\n",
    "        return 0\n",
    "    return abs(x-y)/max(abs(x), abs(y))\n",
    "relative_diff = np.vectorize(relative_diff)\n",
    "\n",
    "def absolute_diff(x: ArrayLike, y: ArrayLike):\n",
    "    \"\"\"Absolute difference\"\"\"\n",
    "    return np.abs(x-y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function\n",
    "\n",
    "Here, we choose a function where we know analytically what the fourier integral is. Of course, in a real setting, you would choose a function without an analytically computable fourier integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sqrt(f: ArrayLike) -> ArrayLike:\n",
    "    return 1 / np.sqrt(2*np.pi*f)\n",
    "\n",
    "# The analytic fourier integral of inv_sqrt\n",
    "def inv_sqrt_analytic_integral(t: ArrayLike):\n",
    "    return np.sqrt(np.pi / (2 * t))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define frequencies and times\n",
    "Define some frequencies as a starting point for the algorithm. It is possible to do as is done here and only provide two frequencies, and still get decent results. It is however *recommended* to follow the following two guidelines when giving an initial frequency array:\n",
    "1. **Make sure all important features of the function are within the endpoints of your array**. The algorithm does a simple scan to higher and lower frequencies, but it is better at adding helpful points to the interior of the initial range.\n",
    "2. **Try to supply a good initial guess of frequencies** that capture the important features of the function. The algorithm starts with doing a rough scan to search for features, then gradually hones in on a finer mesh around the features it finds. However, if there is too much space between the points in the starting array, the rough scan might miss important features.\n",
    "\n",
    "Examples of the effect of these guidelines will be showed below.\n",
    "\n",
    "Also define the times you want to calculate the integral for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only give 2 frequencies\n",
    "# In doing so, we break with the second guideline\n",
    "# We still try to follow the first guideline by having the frequencies span a huge range.\n",
    "initial_frequencies = (1e-10, 1e20)\n",
    "\n",
    "times = np.logspace(-15, 0, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define interpolation error metric\n",
    "\n",
    "A function must be provided that reduces the output interpolation error down to 1D (only the frequency dimension), and applies some error metric. For functions with 1D outputs, this will most often just be the absolute difference between the function and the interpolant. For multidimensional outputs, it might for example be the rms error at each frequency, the maximum error at each frequency, or some weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_error_metric = lambda func_output, interpolant_output: np.abs(func_output - interpolant_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an example, if (say) the function to be transformed had the signature $f : \\reals \\rightarrow \\reals^4$, running the function on a frequency array of size `N` yields a `func_output` array of shape `(N, 4)`. The algorithm will also make an interpolant array of the same size. The error metric function must take in these two arrays and produce an array of shape `(N,)` (i.e. 1D).\n",
    "\n",
    "If you wanted to use the rms error, it would look something like this:\n",
    "\n",
    "```python\n",
    "# Long version\n",
    "def interpolation_error_metric(func_output, interpolant_output):\n",
    "    error = interpolant_output - func_output\n",
    "    squared_error = error**2\n",
    "    mean_squared_error = np.mean(squared_error, axis=1) # This stage changes the shape from (N, 4) to # (N,)\n",
    "    rms_error = np.sqrt(mean_squared_error)\n",
    "    return rms_error\n",
    "\n",
    "# Short version\n",
    "interp_err_metric = lambda func_output, interpolant_output: np.sqrt(np.mean((func_output - interpolant_output)**2, axis=1))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use adaptive algorithm to compute fourier integral\n",
    "This should take 10-20 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies, func_arr = nft.improve_frequency_range(\n",
    "    times=times,\n",
    "    initial_frequencies=initial_frequencies,\n",
    "    func=inv_sqrt,\n",
    "    interpolation_error_metric=interpolation_error_metric,\n",
    "    absolute_integral_tolerance=1e0, # The absolute tolerance the algorithm tries to get the error below\n",
    "    frequency_bound_scan_logstep=2**0.2, # The multiplicative step size used to scan for higher and lower frequencies to add\n",
    "    bisection_mode_condition=None # None (the default) here gives only logarithmic bisection when adding internal points\n",
    ")\n",
    "\n",
    "transform_arr = nft.fourier_integral_fixed_sampling(\n",
    "    times=times,\n",
    "    frequencies=frequencies,\n",
    "    func_values=func_arr,\n",
    "    pos_inf_correction_term=True,\n",
    "    neg_inf_correction_term=False,\n",
    "    interpolation=\"pchip\"\n",
    ")\n",
    "\n",
    "# The two steps above are also combined into the function fourier_integral_adaptive, if one is not interested in the frequencies and func_values used for the Fourier integral itself.\n",
    "# There is no difference in run time between running the two functions separately or the combined function.\n",
    "\n",
    "# Also make an array of the analytically expected values, for comparison\n",
    "transform_arr_analytic = inv_sqrt_analytic_integral(times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The final number of frequencies: {len(frequencies)}, spanning from {frequencies[0]} to {frequencies[-1]}\")\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1,2, figsize=(14,5))\n",
    "ax2 = ax1.twinx()\n",
    "ax4 = ax3.twinx()\n",
    "\n",
    "# Logarithmic scale\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_title(\"Logarithmic frequency scale\")\n",
    "bins = np.geomspace(frequencies[0], frequencies[-1], 50)\n",
    "\n",
    "ax1.hist(frequencies, bins=bins, label=\"#frequencies\")\n",
    "ax1.set_ylabel(\"# frequencies in refined array\")\n",
    "ax1.set_xlabel(\"frequency [a.u.]\")\n",
    "\n",
    "ax2.plot(frequencies[::100], inv_sqrt(frequencies[::100]), \"r\", label=\"func(frequencies)\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_ylabel(\"func(frequencies)\")\n",
    "\n",
    "# Linear scale\n",
    "ax3.set_xscale(\"linear\")\n",
    "ax3.set_title(\"Linear frequency scale\")\n",
    "bins = np.linspace(frequencies[0], frequencies[-1], 50) \n",
    "\n",
    "ax3.hist(frequencies, bins=bins)\n",
    "ax3.set_ylabel(\"# frequencies in refined array\")\n",
    "ax3.set_xlabel(\"frequency [a.u.]\")\n",
    "\n",
    "ax4.plot(frequencies[::100], inv_sqrt(frequencies[::100]), \"r\")\n",
    "ax4.set_yscale(\"log\")\n",
    "ax4.set_ylabel(\"func(frequencies)\")\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select component to plot, feel free to change np.real to e.g. np.imag, np.abs, or np.angle\n",
    "f1 = np.real(transform_arr)\n",
    "f2 = np.real(transform_arr_analytic)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n",
    "ax1.plot(times, f1, \"-o\", markersize=4, label=\"neffint\")\n",
    "ax1.plot(times, f2, \"-o\", markersize=4, label=\"analytic\")\n",
    "ax1.semilogx() # Feel free to change to ax1.loglog\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Fourier transform outputs\")\n",
    "ax1.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "for ax, diff in [(ax2, relative_diff), (ax3, absolute_diff)]:\n",
    "    ax.plot(times, diff(f1, f2), \"-o\", markersize=4)\n",
    "    ax.loglog()\n",
    "    ax.set_title(diff.__doc__)\n",
    "    ax.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "ax3.hlines(1e0, times[0], times[-1], \"r\", linestyles=\"dotted\", label=\"Tolerance\")\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation: More initial frequencies\n",
    "\n",
    "Here, the same calculations are performed, just with more starting frequencies fed into the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same frequency range as before, but now with 1000 log-evenly spread over the range.\n",
    "initial_frequencies_finer = np.geomspace(initial_frequencies[0], initial_frequencies[1], 1000)\n",
    "\n",
    "frequencies_finer, func_arr_finer = nft.improve_frequency_range(\n",
    "    times=times,\n",
    "    initial_frequencies=initial_frequencies_finer,\n",
    "    func=inv_sqrt,\n",
    "    interpolation_error_metric=interpolation_error_metric,\n",
    "    absolute_integral_tolerance=1e0, # The absolute tolerance the algorithm tries to get the error below\n",
    "    frequency_bound_scan_logstep=2**0.2, # The multiplicative step size used to scan for higher and lower frequencies to add\n",
    "    bisection_mode_condition=None # None (the default) here gives only logarithmic bisection when adding internal points\n",
    ")\n",
    "\n",
    "transform_arr_finer = nft.fourier_integral_fixed_sampling(\n",
    "        times=times,\n",
    "        frequencies=frequencies_finer,\n",
    "        func_values=func_arr_finer,\n",
    "        pos_inf_correction_term=True,\n",
    "        neg_inf_correction_term=False,\n",
    "        interpolation=\"pchip\"\n",
    "    )\n",
    "\n",
    "print(f\"Final number of frequencies: {len(frequencies_finer)}, spanning from {frequencies_finer[0]} to {frequencies_finer[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likely the algorithm also ran slightlyt faster now, as the input frequencies were better. Let's plot the results. This time we will also compare with the transform using just the initial frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_arr_initial_frequencies_finer = nft.fourier_integral_fixed_sampling(\n",
    "    times=times,\n",
    "    frequencies=initial_frequencies_finer,\n",
    "    func_values=inv_sqrt(initial_frequencies_finer),\n",
    "    pos_inf_correction_term=True,\n",
    "    neg_inf_correction_term=False,\n",
    "    interpolation=\"pchip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select component to plot, feel free to change np.real to e.g. np.imag, np.abs, or np.angle\n",
    "f1 = np.real(transform_arr_finer)\n",
    "f2 = np.real(transform_arr_initial_frequencies_finer)\n",
    "f3 = np.real(transform_arr_analytic)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,5))\n",
    "ax1.plot(times, f1, \"-o\", markersize=4, label=\"neffint adaptive\")\n",
    "ax1.plot(times, f2, \"-o\", markersize=4, label=\"neffint initial\")\n",
    "ax1.plot(times, f3, \"-o\", markersize=4, label=\"analytic\")\n",
    "ax1.semilogx() # Feel free to change to ax1.loglog\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Fourier transform outputs\")\n",
    "ax1.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "ax3.hlines(1e0, times[0], times[-1], \"r\", linestyles=\"dotted\", label=\"Tolerance\")\n",
    "for ax, diff in [(ax2, relative_diff), (ax3, absolute_diff)]:\n",
    "    ax.plot(times, diff(f1, f3), \"-o\", markersize=4, label=\"adaptive vs analytic\")\n",
    "    ax.plot(times, diff(f2, f3), \"-o\", markersize=4, label=\"initial vs analytic\")\n",
    "    ax.loglog()\n",
    "    ax.legend()\n",
    "    ax.set_title(diff.__doc__)\n",
    "    ax.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plots, the results were more accurate now that we used more initial frequencies. For example, at $t = 10^0 \\text{s}$, we now get $<10^{-4}$ relative error, whereas it was close to $10^{-2}$ before. The plot also shows how the adaptive algorithm improves the frequency range given as input. When looking at the absolute error, we see that the initial frequencies gave an error above $10^0$ in the output, while the improved frequencies stay below $10^0$ over the entire time range. This is to be expected, since the tolerance was set to `1e0`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation: Bad initial frequency endpoints\n",
    "\n",
    "As mentioned, the algorithm is much better at adding points to the interior of the initial frequency range than the exterior. To demonstrate this, here is an example with a much shorter initial range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_frequencies_short = (1e10, 1e20) # Original was (1e-10, 1e20)\n",
    "\n",
    "frequencies_short, func_arr_short = nft.improve_frequency_range(\n",
    "    times=times,\n",
    "    initial_frequencies=initial_frequencies_short,\n",
    "    func=inv_sqrt,\n",
    "    interpolation_error_metric=interpolation_error_metric,\n",
    "    absolute_integral_tolerance=1e0, # The absolute tolerance the algorithm tries to get the error below\n",
    "    frequency_bound_scan_logstep=2**0.2, # The multiplicative step size used to scan for higher and lower frequencies to add\n",
    "    bisection_mode_condition=None # None (the default) here gives only logarithmic bisection when adding internal points\n",
    ")\n",
    "\n",
    "transform_arr_short = nft.fourier_integral_fixed_sampling(\n",
    "        times=times,\n",
    "        frequencies=frequencies_short,\n",
    "        func_values=func_arr_short,\n",
    "        pos_inf_correction_term=True,\n",
    "        neg_inf_correction_term=False,\n",
    "        interpolation=\"pchip\"\n",
    "    )\n",
    "\n",
    "print(f\"Final number of frequencies: {len(frequencies_short)}, spanning from {frequencies_short[0]} to {frequencies_short[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select component to plot, feel free to change np.real to e.g. np.imag, np.abs, or np.angle\n",
    "f1 = np.real(transform_arr_short)\n",
    "f2 = np.real(transform_arr_analytic)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(14,5))\n",
    "ax1.plot(times, f1, \"-o\", markersize=4, label=\"neffint adaptive\")\n",
    "ax1.plot(times, f2, \"-o\", markersize=4, label=\"analytic\")\n",
    "ax1.semilogx() # Feel free to change to ax1.loglog\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Fourier transform outputs\")\n",
    "ax1.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "ax3.hlines(1e0, times[0], times[-1], \"r\", linestyles=\"dotted\", label=\"Tolerance\")\n",
    "for ax, diff in [(ax2, relative_diff), (ax3, absolute_diff)]:\n",
    "    ax.plot(times, diff(f1, f3), \"-o\", markersize=4, label=\"adaptive vs analytic\")\n",
    "    ax.loglog()\n",
    "    ax.legend()\n",
    "    ax.set_title(diff.__doc__)\n",
    "    ax.set_xlabel(\"t [a.u.]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the results are much worse than both previous rounds. The absolute error is even above the tolerance of $10^0$ for the entire range! The relative error is also $\\approx 2$ orders of magnitude higher than the first calculation as well. It is also worth noting that almost all the final frequencies of the first calculation would fall within the range used here, so the few frequencies it missed outside it's initial range made a big impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neffint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
